{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Black Teacher Archvive - School Finder\n",
    "##Inputs historical school listings and identifies current locations\n",
    "##https://curiosity.lib.harvard.edu/black-teacher-archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEONAMES Search Script\n",
    "##Takes schools table and returns lat/long + GeoNames name\n",
    "###Test Input @: \n",
    "###https://docs.google.com/spreadsheets/d/1VHF2QYGxQdMeTR1kGhoncK21sidmrk6Mis0h5VY4AiE/edit#gid=2140551881\n",
    "\n",
    "#Libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "#API Use Counter\n",
    "request_counter = 0\n",
    "def increment_request_counter():\n",
    "    global request_counter\n",
    "    request_counter += 1\n",
    "\n",
    "def is_county_in_hierarchy(username, geonameId, target_county):\n",
    "    hierarchy_url = \"http://api.geonames.org/hierarchyJSON\"\n",
    "    params = {\n",
    "        'geonameId': geonameId,\n",
    "        'username': username\n",
    "    }\n",
    "    increment_request_counter()  # Increment the request counter\n",
    "    response = requests.get(hierarchy_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        hierarchy_data = response.json()\n",
    "        for place in hierarchy_data.get('geonames', []):\n",
    "            if target_county.lower() in place.get('name', '').lower():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_school_info(username, school, target_county, fuzzy):\n",
    "    global request_counter\n",
    "    # Extract the first word of the school's name for broader search\n",
    "    base_school_name = school.split()[0]\n",
    "    school_name_extension = school.split()[1:]\n",
    "    s = \" \"\n",
    "    school_name_extension = s.join(school_name_extension)\n",
    "    # Educational institution types to include in the search\n",
    "    institution_types = [school_name_extension, \"School\", \"College\", \"Academy\", school_name_extension + \" (historical)\"]\n",
    "    \n",
    "    # Attempt searches for each institution type with the base school name\n",
    "    for institution_type in institution_types:\n",
    "        print(\"Searching \",f\"{base_school_name} {institution_type}\",\"\\n\")\n",
    "        search_url = \"http://api.geonames.org/searchJSON\"\n",
    "        search_params = {\n",
    "            'q': f\"{base_school_name} {institution_type}\",\n",
    "            'country': 'US',\n",
    "            'adminCode1': 'TN',\n",
    "            'username': username,\n",
    "            'fuzzy': fuzzy,\n",
    "            'maxRows': 100\n",
    "        }\n",
    "        increment_request_counter()\n",
    "        search_response = requests.get(search_url, params=search_params)\n",
    "        if search_response.status_code == 200:\n",
    "            search_data = search_response.json()\n",
    "            for result in search_data.get('geonames', []):\n",
    "                if is_county_in_hierarchy(username, result['geonameId'], target_county) and result.get('fcode', '') == 'SCH':\n",
    "                    return result['lat'], result['lng'], result.get('name', ''), result.get('fcode', ''), result.get('fcl', '')\n",
    "\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# Load your CSV file into a DataFrame\n",
    "csv_file_path = '...csv'  # Update with the actual path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "geonames_username = '...'  # Update with your actual GeoNames username\n",
    "fuzzy_value = 0.7  # Adjust based on your needs\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    school = row['SCHOOL']\n",
    "    county = row['COUNTY']\n",
    "    \n",
    "    lat, lng, geoname_name, fcode, fcl = get_school_info(geonames_username, school, county, fuzzy_value)\n",
    "    if lat and lng:\n",
    "        df.at[index, 'Latitude'] = lat\n",
    "        df.at[index, 'Longitude'] = lng\n",
    "        df.at[index, 'GeoNames Name'] = geoname_name\n",
    "        df.at[index, 'Feature Code'] = fcode\n",
    "        df.at[index, 'Feature Class'] = fcl\n",
    "    else:\n",
    "        print(f\"No valid results found for {school} in {county}\")\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "updated_csv_file_path = '...csv'  # Update with the actual path\n",
    "df.to_csv(updated_csv_file_path, index=False)\n",
    "print(f\"Updated CSV file saved to {updated_csv_file_path}.\")\n",
    "\n",
    "# Print the total number of requests made to the GeoNames API\n",
    "print(f\"Total requests made to GeoNames API: {request_counter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historical Marker Database Search\n",
    "##Searches HMDB(.org) for remaining (i.e., missing) schools and returns lat/long\n",
    "\n",
    "#Libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "\n",
    "# Function to parse latitude and longitude from text\n",
    "def parse_lat_long(text):\n",
    "    text = text.replace('′', '').replace('″', '').replace('°', '')\n",
    "    parts = text.split(',')\n",
    "    lat_text = parts[0].strip()\n",
    "    lng_text = parts[1].strip()\n",
    "\n",
    "    lat_deg, lat_min = map(float, re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", lat_text))\n",
    "    lat = lat_deg + (lat_min / 60)\n",
    "    if 'S' in lat_text:\n",
    "        lat = -lat\n",
    "\n",
    "    lng_deg, lng_min = map(float, re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", lng_text))\n",
    "    lng = lng_deg + (lng_min / 60)\n",
    "    if 'W' in lng_text:\n",
    "        lng = -lng\n",
    "\n",
    "    return lat, lng\n",
    "\n",
    "# Function to extract latitude and longitude from the HMDB page\n",
    "def extract_lat_long(driver):\n",
    "    try:\n",
    "        location_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"mainblock\"]/article/span[contains(text(), \"Location.\")]'))\n",
    "        )\n",
    "        location_text = location_element.find_element(By.XPATH, 'following-sibling::node()').text.strip()\n",
    "        print(f\"Location text: {location_text}\")\n",
    "        \n",
    "        lat_lng_match = re.search(r'([-\\d.]+°\\s*[\\d.]+′\\s*[NS]),\\s*([-\\d.]+°\\s*[\\d.]+′\\s*[EW])', location_text)\n",
    "        if lat_lng_match:\n",
    "            lat_lng_text = lat_lng_match.group()\n",
    "            print(f\"Lat/Long text: {lat_lng_text}\")\n",
    "            print('\\n')\n",
    "            lat, lng = parse_lat_long(lat_lng_text)\n",
    "            return lat, lng\n",
    "        \n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "       # print(f\"Error extracting lat/long: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform the HMDB search and extract lat/long\n",
    "def perform_hmdb_search(driver, keyword, country, state, county):\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    show_filters = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'show filters')]\")))\n",
    "    show_filters.click()\n",
    "    \n",
    "    keyword_search_label = wait.until(EC.presence_of_element_located((By.XPATH, \"//h4[text()='Keyword Search']\")))\n",
    "    search_box = keyword_search_label.find_element(By.XPATH, \"./following-sibling::div//input[@type='text']\")\n",
    "    search_box.send_keys(keyword)\n",
    "    \n",
    "    country_input = wait.until(EC.visibility_of_element_located((By.NAME, \"FilterCountry\")))\n",
    "    country_input.send_keys(country)\n",
    "    \n",
    "    state_input = wait.until(EC.visibility_of_element_located((By.NAME, \"FilterState\")))\n",
    "    state_input.send_keys(state)\n",
    "    \n",
    "    county_input = wait.until(EC.visibility_of_element_located((By.NAME, \"FilterCounty\")))\n",
    "    county_input.send_keys(county)\n",
    "    \n",
    "    search_button = wait.until(EC.element_to_be_clickable((By.ID, \"TheButton1\")))\n",
    "    search_button.click()\n",
    "    \n",
    "    time.sleep(2)  # Adjust the sleep time if necessary\n",
    "    headers = driver.find_elements(By.TAG_NAME, \"h1\")\n",
    "    headers += driver.find_elements(By.TAG_NAME, \"h2\")\n",
    "    \n",
    "    for header in headers:\n",
    "        if keyword.lower() in header.text.lower():\n",
    "            print(f\"Found match: {header.text}\")  # Debugging statement\n",
    "            lat, lng = extract_lat_long(driver)\n",
    "            return header.text, lat, lng\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "# Load your CSV file into a DataFrame\n",
    "csv_file_path = '...csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize the Selenium WebDriver\n",
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.hmdb.org/search.asp')\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    school_name = row['SCHOOL']\n",
    "    country = 'United States'\n",
    "    state = row['STATE'] if 'STATE' in row else 'Tennessee'\n",
    "    county = row['COUNTY']\n",
    "    \n",
    "    if pd.isna(row['GeoNames Name']):  # Check if GeoNames entry is missing\n",
    "        base_school_name = \" \".join(school_name.split()[:2])\n",
    "        header, lat, lng = perform_hmdb_search(driver, base_school_name, country, state, county)\n",
    "        if header and lat and lng:\n",
    "            df.at[index, 'HMDB Name'] = header\n",
    "            df.at[index, 'Latitude'] = lat\n",
    "            df.at[index, 'Longitude'] = lng\n",
    "        else:\n",
    "            print(f\"No matches found for {school_name} in {county}, {state}\")\n",
    "            print('\\n')\n",
    "            \n",
    "        driver.get('https://www.hmdb.org/search.asp')\n",
    "        time.sleep(2)  # Give some time for the page to load properly\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Time elapsed: {elapsed_time} seconds\")\n",
    "\n",
    "updated_csv_file_path = '...csv'\n",
    "df.to_csv(updated_csv_file_path, index=False)\n",
    "print(f\"Updated CSV file saved to {updated_csv_file_path}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28047bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#Find Nearest Address:\n",
    "##https://www.geonames.org/maps/us-reverse-geocoder.html#findNearestAddress\n",
    "#Table image OCR?\n",
    "#Process \"State Association Black Schools\" docs\n",
    "#update github README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cook 2024\n",
    "###mncook.net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
