{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Black Teacher Archvive - School Finder\n",
    "##Inputs historical school listings and identifies current locations\n",
    "##https://curiosity.lib.harvard.edu/black-teacher-archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup - libraries, globals, I/O, references\n",
    "\n",
    "##Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "\n",
    "##Global variables\n",
    "prepped_extension = \"_PREPPED\"\n",
    "geonames_extension = \"_GEONAMES\"\n",
    "hmdb_extension = \"_HMDB\"\n",
    "geonames_username = \"...\" #GeoNames username\n",
    "fuzzy_value = 0.7 #based on the Damerau-Levenshtein\n",
    "\n",
    "##I/O\n",
    "input_csv_path = '...'  # Update with the actual path\n",
    "input_parent_dir = os.path.dirname(input_csv_path)\n",
    "processing_output_dir = os.path.join(input_parent_dir, 'Processing')\n",
    "hmdb_output_dir = os.path.join(input_parent_dir, 'Output')\n",
    "\n",
    "if not os.path.exists(processing_output_dir):\n",
    "    os.makedirs(processing_output_dir)\n",
    "    print(f\"Created directory: {processing_output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {processing_output_dir}\")\n",
    "\n",
    "if not os.path.exists(hmdb_output_dir):\n",
    "    os.makedirs(hmdb_output_dir)\n",
    "    print(f\"Created directory: {hmdb_output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {hmdb_output_dir}\")\n",
    "\n",
    "prepped_output_csv = os.path.join(processing_output_dir, os.path.splitext(os.path.basename(input_csv_path))[0] + prepped_extension + \".csv\")\n",
    "geonames_output_csv = os.path.join(processing_output_dir, os.path.splitext(os.path.basename(input_csv_path))[0] + geonames_extension + \".csv\")\n",
    "hmdb_output_csv = os.path.join(hmdb_output_dir, os.path.splitext(os.path.basename(input_csv_path))[0] + hmdb_extension + \".csv\")\n",
    "\n",
    "print(f\"Prepped output CSV path: {prepped_output_csv}\")\n",
    "print(f\"GeoNames output CSV path: {geonames_output_csv}\")\n",
    "print(f\"HMDB output CSV path: {hmdb_output_csv}\")\n",
    "\n",
    "##References\n",
    "state_abbreviations = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', \n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA', \n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', \n",
    "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', \n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', \n",
    "    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', \n",
    "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', \n",
    "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', \n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', \n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1564d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep\n",
    "##Takes arbitrary historical table input and outputs standardized table for downstream searches\n",
    "def standardize_csv(input_csv_path, prepped_output_csv):\n",
    "    # Parse the input filename to get the state\n",
    "    filename = os.path.basename(input_csv_path)\n",
    "    state = filename.split()[0]\n",
    "    print(f\"DEBUG: Extracted state from filename: {state}\", \" processing now...\")  # Debug log for state name\n",
    "    \n",
    "    # Load the input CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Define the template columns and corresponding regular expressions for matching\n",
    "    template_columns = {\n",
    "        \"State\": re.compile(r'\\bstate\\b', re.IGNORECASE),\n",
    "        \"County\": re.compile(r'\\bcounty\\b', re.IGNORECASE),\n",
    "        \"City\": re.compile(r'\\b(city|location)\\b', re.IGNORECASE),\n",
    "        \"School\": re.compile(r'\\b(school|name\\s*of\\s*school|school\\s*name|institution)\\b', re.IGNORECASE),\n",
    "        \"Latitude\": re.compile(r'\\blatitude\\b', re.IGNORECASE),\n",
    "        \"Longitude\": re.compile(r'\\blongitude\\b', re.IGNORECASE),\n",
    "        \"GeoNames Name\": re.compile(r'\\bgeonames\\s*name\\b', re.IGNORECASE),\n",
    "        \"GeoNames Feature Code\": re.compile(r'\\bgeonames\\s*feature\\s*code\\b', re.IGNORECASE),\n",
    "        \"HMDB Name\": re.compile(r'\\bhmdb\\s*name\\b', re.IGNORECASE)\n",
    "    }\n",
    "    \n",
    "    # Create a new DataFrame with the required template columns\n",
    "    standardized_df = pd.DataFrame(columns=template_columns.keys())\n",
    "    \n",
    "    # Copy relevant columns from the input DataFrame to the new DataFrame\n",
    "    for template_col, regex in template_columns.items():\n",
    "        if template_col != \"State\":  # Skip the \"State\" column as it's handled separately\n",
    "            matching_columns = [col for col in df.columns if regex.search(col)]\n",
    "            if matching_columns:\n",
    "                standardized_df[template_col] = df[matching_columns[0]]\n",
    "    \n",
    "    # Set the \"State\" column with the parsed state value for each row\n",
    "    standardized_df[\"State\"] = state\n",
    "    \n",
    "    # Save the standardized DataFrame to the output CSV file\n",
    "    standardized_df.to_csv(prepped_output_csv, index=False)\n",
    "    print(f\"Standardized CSV file saved to {prepped_output_csv}\")\n",
    "\n",
    "standardize_csv(input_csv_path, prepped_output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEONAMES Search Script\n",
    "##Takes schools table and returns lat/long + GeoNames name + feature code (e.g., SCH)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to increment API request counter\n",
    "request_counter = 0\n",
    "def increment_request_counter():\n",
    "    global request_counter\n",
    "    request_counter += 1\n",
    "\n",
    "# Function to check if county is in GeoNames hierarchy\n",
    "def is_county_in_hierarchy(username, geonameId, target_county):\n",
    "    if pd.isna(target_county):\n",
    "        return False\n",
    "    hierarchy_url = \"http://api.geonames.org/hierarchyJSON\"\n",
    "    params = {\n",
    "        'geonameId': geonameId,\n",
    "        'username': username\n",
    "    }\n",
    "    increment_request_counter()\n",
    "    response = requests.get(hierarchy_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        hierarchy_data = response.json()\n",
    "        for place in hierarchy_data.get('geonames', []):\n",
    "            if target_county.lower() in place.get('name', '').lower():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to check if city is in GeoNames hierarchy\n",
    "def is_city_in_hierarchy(username, geonameId, target_city):\n",
    "    if pd.isna(target_city):\n",
    "        return False\n",
    "    hierarchy_url = \"http://api.geonames.org/hierarchyJSON\"\n",
    "    params = {\n",
    "        'geonameId': geonameId,\n",
    "        'username': username\n",
    "    }\n",
    "    increment_request_counter()\n",
    "    response = requests.get(hierarchy_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        hierarchy_data = response.json()\n",
    "        for place in hierarchy_data.get('geonames', []):\n",
    "            if target_city.lower() in place.get('name', '').lower():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to get school information from GeoNames\n",
    "def get_school_info(username, school, target_county, target_city, state, fuzzy, state_abbreviations):\n",
    "    global request_counter\n",
    "    # Extract the first word of the school's name for broader search\n",
    "    base_school_name = school.split()[0]\n",
    "    school_name_extension = school.split()[1:]\n",
    "    s = \" \"\n",
    "    school_name_extension = s.join(school_name_extension)\n",
    "    # Educational institution types to include in the search\n",
    "    institution_types = [school_name_extension, \"School\", \"College\", \"Academy\", school_name_extension + \" (historical)\"]\n",
    "    \n",
    "    # Get state abbreviation\n",
    "    state_abbr = state_abbreviations.get(state)  # Assume state abbreviation is available\n",
    "    \n",
    "    # Attempt searches for each institution type with the base school name\n",
    "    for institution_type in institution_types:\n",
    "        print(f\"Searching {base_school_name} {institution_type} in {state_abbr}\\n\")\n",
    "        search_url = \"http://api.geonames.org/searchJSON\"\n",
    "        search_params = {\n",
    "            'q': f\"{base_school_name} {institution_type}\",\n",
    "            'country': 'US',\n",
    "            'adminCode1': state_abbr,\n",
    "            'username': username,\n",
    "            'fuzzy': fuzzy,\n",
    "            'maxRows': 100\n",
    "        }\n",
    "        increment_request_counter()\n",
    "        search_response = requests.get(search_url, params=search_params)\n",
    "        if search_response.status_code == 200:\n",
    "            search_data = search_response.json()\n",
    "            for result in search_data.get('geonames', []):\n",
    "                if ((target_county and is_county_in_hierarchy(username, result['geonameId'], target_county)) or\n",
    "                    (target_city and is_city_in_hierarchy(username, result['geonameId'], target_city))) and result.get('fcode', '') == 'SCH':\n",
    "                    return result['lat'], result['lng'], result.get('name', ''), result.get('fcode', '')\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "# Load your CSV file into a DataFrame\n",
    "df = pd.read_csv(prepped_output_csv)\n",
    "\n",
    "# Initialize counters for results\n",
    "county_matches = 0\n",
    "city_matches = 0\n",
    "total_matches = 0\n",
    "total_matches_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    school = row['School']\n",
    "    county = row['County'] if 'County' in row else None\n",
    "    city = row['City'] if 'City' in row else None\n",
    "    state = row['State']\n",
    "    \n",
    "    lat, lng, geoname_name, fcode = get_school_info(geonames_username, school, county, city, state, fuzzy_value, state_abbreviations)\n",
    "    if lat and lng:\n",
    "        df.at[index, 'Latitude'] = lat\n",
    "        df.at[index, 'Longitude'] = lng\n",
    "        df.at[index, 'GeoNames Name'] = geoname_name\n",
    "        df.at[index, 'GeoNames Feature Code'] = fcode\n",
    "        total_matches += 1\n",
    "        total_matches_list.append(geoname_name)\n",
    "        if county:\n",
    "            county_matches += 1\n",
    "        elif city:\n",
    "            city_matches += 1\n",
    "    else:\n",
    "        print(f\"No valid results found for {school} in {county or city}, {state}\")\n",
    "\n",
    "# Ensure the DataFrame has all the required columns\n",
    "required_columns = [\"State\", \"County\", \"City\", \"School\", \"Latitude\", \"Longitude\", \"GeoNames Name\", \"GeoNames Feature Code\", \"HMDB Name\"]\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(geonames_output_csv, index=False)\n",
    "print(f\"Updated CSV file saved to {geonames_output_csv}.\")\n",
    "\n",
    "# Print the total number of requests made to the GeoNames API\n",
    "print(f\"Total requests made to GeoNames API: {request_counter}\")\n",
    "\n",
    "# Log the time elapsed\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Time elapsed: {elapsed_time} seconds\")\n",
    "\n",
    "# Log the results\n",
    "print(f\"Total matches: {total_matches}\")\n",
    "for match in total_matches_list:\n",
    "    print(f\"Match found: {match}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39537e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Function to parse latitude and longitude from text\n",
    "def parse_lat_long(text):\n",
    "    text = text.replace('′', '').replace('″', '').replace('°', '')\n",
    "    parts = text.split(',')\n",
    "    lat_text = parts[0].strip()\n",
    "    lng_text = parts[1].strip()\n",
    "\n",
    "    lat_deg, lat_min = map(float, re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", lat_text))\n",
    "    lat = lat_deg + (lat_min / 60)\n",
    "    if 'S' in lat_text:\n",
    "        lat = -lat\n",
    "\n",
    "    lng_deg, lng_min = map(float, re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", lng_text))\n",
    "    lng = lng_deg + (lng_min / 60)\n",
    "    if 'W' in lng_text:\n",
    "        lng = -lng\n",
    "\n",
    "    return lat, lng\n",
    "\n",
    "# Function to extract latitude and longitude from the HMDB page\n",
    "def extract_lat_long(driver):\n",
    "    try:\n",
    "        location_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"mainblock\"]/article/span[contains(text(), \"Location.\")]'))\n",
    "        )\n",
    "        location_text = location_element.find_element(By.XPATH, 'following-sibling::node()').text.strip()\n",
    "        print(f\"Location text: {location_text}\")\n",
    "        \n",
    "        lat_lng_match = re.search(r'([-\\d.]+°\\s*[\\d.]+′\\s*[NS]),\\s*([-\\d.]+°\\s*[\\d.]+′\\s*[EW])', location_text)\n",
    "        if lat_lng_match:\n",
    "            lat_lng_text = lat_lng_match.group()\n",
    "            print(f\"Lat/Long text: {lat_lng_text}\")\n",
    "            print('\\n')\n",
    "            lat, lng = parse_lat_long(lat_lng_text)\n",
    "            return lat, lng\n",
    "        \n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "       # print(f\"Error extracting lat/long: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform the HMDB search and extract lat/long\n",
    "def perform_hmdb_search(driver, keyword, country, state, county, city):\n",
    "    wait = WebDriverWait(driver, 3)\n",
    "    \n",
    "    show_filters = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'show filters')]\")))\n",
    "    show_filters.click()\n",
    "    \n",
    "    keyword_search_label = wait.until(EC.presence_of_element_located((By.XPATH, \"//h4[text()='Keyword Search']\")))\n",
    "    search_box = keyword_search_label.find_element(By.XPATH, \"./following-sibling::div//input[@type='text']\")\n",
    "    search_box.send_keys(keyword)\n",
    "    \n",
    "    country_input = wait.until(EC.visibility_of_element_located((By.NAME, \"FilterCountry\")))\n",
    "    country_input.send_keys(country)\n",
    "    \n",
    "    state_input = wait.until(EC.visibility_of_element_located((By.NAME, \"FilterState\")))\n",
    "    state_input.send_keys(state)\n",
    "    \n",
    "    if county:\n",
    "        county_input = wait.until(EC.visibility_of_element_located((By.NAME, \"FilterCounty\")))\n",
    "        county_input.send_keys(county)\n",
    "    \n",
    "    if city:\n",
    "        city_input = wait.until(EC.visibility_of_element_located((By.NAME, \"FilterTown\")))\n",
    "        city_input.send_keys(city)\n",
    "    \n",
    "    search_button = wait.until(EC.element_to_be_clickable((By.ID, \"TheButton1\")))\n",
    "    search_button.click()\n",
    "    \n",
    "    time.sleep(2)  # Adjust the sleep time if necessary\n",
    "    headers = driver.find_elements(By.TAG_NAME, \"h1\")\n",
    "    headers += driver.find_elements(By.TAG_NAME, \"h2\")\n",
    "    \n",
    "    for header in headers:\n",
    "        if keyword.lower() in header.text.lower():\n",
    "            print(f\"Found match: {header.text}\")  # Debugging statement\n",
    "            lat, lng = extract_lat_long(driver)\n",
    "            return header.text, lat, lng\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "# Load your CSV file into a DataFrame\n",
    "df = pd.read_csv(geonames_output_csv)\n",
    "\n",
    "# Initialize the Selenium WebDriver\n",
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.hmdb.org/search.asp')\n",
    "wait = WebDriverWait(driver, 3)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    school_name = row['School']\n",
    "    country = 'United States'\n",
    "    state = row['State']\n",
    "    county = row['County'] if 'County' in row else None\n",
    "    city = row['City'] if 'City' in row else None\n",
    "    \n",
    "    if pd.isna(row['GeoNames Name']):  # Check if GeoNames entry is missing\n",
    "        base_school_name = \" \".join(school_name.split()[:2])\n",
    "        header, lat, lng = perform_hmdb_search(driver, base_school_name, country, state, county, city)\n",
    "        if header and lat and lng:\n",
    "            df.at[index, 'HMDB Name'] = header\n",
    "            df.at[index, 'Latitude'] = lat\n",
    "            df.at[index, 'Longitude'] = lng\n",
    "        else:\n",
    "            print(f\"No matches found for {school_name} in {county or city}, {state}\")\n",
    "            print('\\n')\n",
    "            \n",
    "        driver.get('https://www.hmdb.org/search.asp')\n",
    "        time.sleep(2)  # Give some time for the page to load properly\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Time elapsed: {elapsed_time} seconds\")\n",
    "\n",
    "df.to_csv(hmdb_output_csv, index=False)\n",
    "print(f\"Updated CSV file saved to {hmdb_output_csv}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28047bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "#Find Nearest Address (GeoNames):\n",
    "##https://www.geonames.org/maps/us-reverse-geocoder.html#findNearestAddress\n",
    "#Select table image OCR tool (\"pre-prep\")\n",
    "#update github README\n",
    "#bypass Cloudflare verification for HMDB search code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cook 2024\n",
    "###mncook.net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
